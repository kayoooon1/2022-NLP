# [CS224n] - 1. Introduction and Word Vector   
## Word Vectors    
* Distributional sementics(의미론)   
    - 단어의 빈도수가 아니라 context를 바탕으로 한 distributional model이 등장하기 시작
* Dense real value vector for each word, represent the meaning of the word
    - 단어가 다른 문서에 등장할 때 잘 유추하기 위함   

<br/>

## Word2vec   
### 1. 개념
* Framework for learning word vectors   
* corpus 말뭉치 (많은 text), create for vector for each word   
* 다른 문맥에서 어떤 단어가 나올까 예측하면서 (= 즉, Center word c와 Context (outside) o의 유사도를 계산해서 c가 주어졌을 때 o가 나올 확률을 구하는)   
* 각 corpus의 위치에서 m이라는 고정 길이 사이즈의 window가 움직일 때마다 주어지는 $w_j$, 그 단어를 예측할 확률을 높이고 싶은 것   
### 2. 과정
![image.png](https://velog.velcdn.com/images%2Ftobigs-text1314%2Fpost%2Fac99757f-3e52-4af5-a5cc-526048421191%2Fimage.png)   
1) 현재 위치 t에 있는 단어를 $W_t$, 주변에 있는 단어를 $W_t + n$, $W_t - n$라고 할 때, 
$P(W_t + n|W_t)$, $P(W_t - n|W_t)$ 구함   
2) 확률 최대화 vector 찾기   
3) 말뭉치(corpus) 안의 모든 단어에 대해 1~2를 적용   
### 3. 계산법   
1. Likelihood   
![image.png](https://velog.velcdn.com/images%2Ftobigs-text1314%2Fpost%2Ff2c90f4e-097d-4d76-94ef-5d448d98a618%2Fimage.png)   
- window에서 context word가 해당 위치에 나타날 확률 곱    

2. Object Function   
![image.png](https://velog.velcdn.com/images%2Ftobigs-text1314%2Fpost%2F7e9752b2-3271-4155-8116-0bb33ebceb7e%2Fimage.png)   
- exp 반대가 log라서 쓴다고

3. $P(o|c)$   
![image.png](https://velog.velcdn.com/images%2Ftobigs-text1314%2Fpost%2Fff926370-b294-44d4-bdc8-a0d63041d713%2FObjective_function.jpg)
![image.png](https://velog.velcdn.com/images%2Ftobigs-text1314%2Fpost%2F083ef83b-b6f8-4b6a-a9c0-8f9b13613f5b%2Fimage.png)   
- word $w$에 대하여 2개의 벡터를 사용:   
    $v_w$는 $w$가 center word, $u_w$는 $w$가 context (outside) word   
- softmax function   

4. Training   
![image.png](https://velog.velcdn.com/images%2Ftobigs-text1314%2Fpost%2F00279469-2251-4f9b-9151-c0a5f021f673%2Fimage.png)
* $theta$ represents **all** model parameters   
* word vector u, v 포함하므로 2dV   


