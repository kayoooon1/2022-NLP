# Word Vectors   
## Word Vectors    
* Distributional sementics(의미론)   
    - 단어의 빈도수가 아니라 context를 바탕으로 한 distributional model이 등장하기 시작
* Dense real value vector for each word, represent the meaning of the word
    - 단어가 다른 문서에 등장할 때 잘 유추하기 위함   

<br/>

## Word2vec   
### 1. 개념
* Framework for learning word vectors   
* corpus 말뭉치 (많은 text), create for vector for each word   
* 다른 문맥에서 어떤 단어가 나올까 예측하면서 (= 즉, Center word c와 o의 유사도를 계산해서 c가 주어졌을 때 o가 나올 확률을 구하는)   
* 각 corpus의 위치에서 m이라는 고정 길이 사이즈의 window가 움직일 때마다 주어지는 $w_j$, 그 단어를 예측할 확률을 높이고 싶은 것   
### 2. 과정
![image.png](https://velog.velcdn.com/images%2Ftobigs-text1314%2Fpost%2Fac99757f-3e52-4af5-a5cc-526048421191%2Fimage.png)   
1) 현재 위치 t에 있는 단어를 $W_t$, 주변에 있는 단어를 $W_t + n$, $W_t - n$라고 할 때, 
$P(W_t + n|W_t)$, $P(W_t - n|W_t)$ 구함   
2) 확률 최대화 vector 찾기   
3) 말뭉치(corpus) 안의 모든 단어에 대해 1~2를 적용   
### 3. 계산법   
1. Likelihood
![image.png](https://velog.velcdn.com/images%2Ftobigs-text1314%2Fpost%2Ff2c90f4e-097d-4d76-94ef-5d448d98a618%2Fimage.png)   
2. Object Function   
![image.png](https://velog.velcdn.com/images%2Ftobigs-text1314%2Fpost%2F7e9752b2-3271-4155-8116-0bb33ebceb7e%2Fimage.png)   
